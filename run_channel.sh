#!/bin/bash
# run_channel.sh — Run dolfinx-rans k-ω channel flow solver
#
# Usage:
#   ./run_channel.sh                       # Serial, canonical channel case (default)
#   ./run_channel.sh 8                     # 8 MPI processes, canonical channel case
#   ./run_channel.sh path/to/config.json   # Serial, custom config
#   ./run_channel.sh 4 path/to/config.json # 4 MPI processes, custom config

set -e  # Exit on error

# ─────────────────────────────────────────────────────────────────────────────
# Configuration
# ─────────────────────────────────────────────────────────────────────────────
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
CONDA_ENV="fenicsx"
DEFAULT_CASE_DIR="$SCRIPT_DIR/channel"
DEFAULT_CONFIG="$DEFAULT_CASE_DIR/channel.json"

write_default_channel_config() {
    mkdir -p "$DEFAULT_CASE_DIR"
    cat > "$DEFAULT_CONFIG" <<'JSON'
{
  "_meta": {
    "purpose": "Canonical single-case run config generated by run_channel.sh",
    "reference": "Nek poiseuille_RANS uses Re=100000 via viscosity=-1e5 in .par",
    "notes": "Delete channel/* to recompute outputs from scratch."
  },
  "geom": {
    "Lx": 1.0,
    "Ly": 2.0,
    "Nx": 192,
    "Ny": 166,
    "mesh_type": "quad",
    "y_first": 0.001604628,
    "growth_rate": 1.0,
    "stretching": "tanh",
    "y_first_tol_rel": 0.2,
    "use_symmetry": false
  },
  "nondim": {
    "Re_tau": 1115.818661288065,
    "use_body_force": true
  },
  "turb": {
    "model": "wilcox2006",
    "beta_star": 0.09,
    "nu_t_max_factor": 2000.0,
    "omega_min": 1.0,
    "k_min": 1e-10,
    "k_max": 20.0,
    "C_lim": 0.0
  },
  "solve": {
    "dt": 0.0002,
    "dt_max": 0.01,
    "dt_growth": 1.05,
    "dt_growth_threshold": 0.8,
    "t_final": 10000.0,
    "max_iter": 1200,
    "steady_tol": 1e-3,
    "enable_physical_convergence": false,
    "physical_u_bulk_rel_tol": 1e-4,
    "physical_tau_wall_rel_tol": 2.5e-3,
    "physical_convergence_start_iter": 10,
    "picard_max": 6,
    "picard_tol": 1e-4,
    "under_relax_k_omega": 0.6,
    "under_relax_nu_t": 0.4,
    "log_interval": 10,
    "snapshot_interval": 50,
    "out_dir": "channel"
  },
  "benchmark": {
    "reference_profile_csv": ""
  }
}
JSON
    cp "$DEFAULT_CONFIG" "$DEFAULT_CASE_DIR/run_config.json"
}

# ─────────────────────────────────────────────────────────────────────────────
# Activate conda environment
# ─────────────────────────────────────────────────────────────────────────────
echo "═══════════════════════════════════════════════════════════════════════"
echo "  dolfinx-rans: RANS k-ω Channel Flow Solver"
echo "═══════════════════════════════════════════════════════════════════════"

# Find and source conda
if [ -f "/opt/homebrew/bin/conda" ]; then
    eval "$(/opt/homebrew/bin/conda shell.bash hook)"
elif [ -f "$HOME/miniforge3/etc/profile.d/conda.sh" ]; then
    source "$HOME/miniforge3/etc/profile.d/conda.sh"
elif [ -f "$HOME/mambaforge/etc/profile.d/conda.sh" ]; then
    source "$HOME/mambaforge/etc/profile.d/conda.sh"
elif [ -f "$HOME/anaconda3/etc/profile.d/conda.sh" ]; then
    source "$HOME/anaconda3/etc/profile.d/conda.sh"
elif [ -f "$HOME/miniconda3/etc/profile.d/conda.sh" ]; then
    source "$HOME/miniconda3/etc/profile.d/conda.sh"
else
    echo "ERROR: Could not find conda installation"
    exit 1
fi

echo "Activating conda environment: $CONDA_ENV"
conda activate "$CONDA_ENV"

# ─────────────────────────────────────────────────────────────────────────────
# Install package if needed
# ─────────────────────────────────────────────────────────────────────────────
if ! python -c "import dolfinx_rans" 2>/dev/null; then
    echo "Installing dolfinx-rans..."
    pip install -e "$SCRIPT_DIR" --quiet
fi

# ─────────────────────────────────────────────────────────────────────────────
# Parse arguments: [NPROCS] [CONFIG]
# ─────────────────────────────────────────────────────────────────────────────
NPROCS=1
CONFIG_ARG=""

# Check if first arg is a number (MPI processes)
if [[ "$1" =~ ^[0-9]+$ ]]; then
    NPROCS="$1"
        CONFIG_ARG="${2:-channel}"
else
    CONFIG_ARG="${1:-channel}"
fi

# Select config file
case "$CONFIG_ARG" in
    channel|canonical|default|nek|highre|re125k)
        write_default_channel_config
        CONFIG="$DEFAULT_CONFIG"
        echo "Running: canonical channel case"
        ;;
    *.json)
        CONFIG="$CONFIG_ARG"
        echo "Running: Custom config $CONFIG"
        ;;
    *)
        echo "ERROR: Unknown config '$CONFIG_ARG'"
        echo "Usage: ./run_channel.sh [NPROCS] [channel|path/to/config.json]"
        exit 1
        ;;
esac

if [ ! -f "$CONFIG" ]; then
    echo "ERROR: Config file not found: $CONFIG"
    exit 1
fi
CONFIG="$(realpath "$CONFIG")"

echo "Config: $CONFIG"
if [ "$NPROCS" -gt 1 ]; then
    echo "MPI processes: $NPROCS"
fi
echo "───────────────────────────────────────────────────────────────────────"

# ─────────────────────────────────────────────────────────────────────────────
# Pre-flight checks
# ─────────────────────────────────────────────────────────────────────────────
echo "Running pre-flight checks..."

# 1. Syntax check
if ! python -m py_compile "$SCRIPT_DIR/src/dolfinx_rans/solver.py" 2>/dev/null; then
    echo "ERROR: Python syntax error in solver.py"
    python -m py_compile "$SCRIPT_DIR/src/dolfinx_rans/solver.py"
    exit 1
fi

# 2. Import check
python -c "
import sys
errors = []

# Core dependencies
try:
    from mpi4py import MPI
except ImportError as e:
    errors.append(f'mpi4py: {e}')

try:
    from petsc4py import PETSc
except ImportError as e:
    errors.append(f'petsc4py: {e}')

try:
    import dolfinx
except ImportError as e:
    errors.append(f'dolfinx: {e}')

# Package import
try:
    import dolfinx_rans
except ImportError as e:
    errors.append(f'dolfinx_rans: {e}')

if errors:
    print('Import errors:')
    for err in errors:
        print(f'  ✗ {err}')
    sys.exit(1)
else:
    print('  ✓ All imports OK')
" || exit 1

# 3. Config validation
python -c "
import json
import sys
with open('$CONFIG') as f:
    cfg = json.load(f)
if 'nondim' not in cfg or 'turb' not in cfg or 'solve' not in cfg:
    print('ERROR: Config missing one of required sections: nondim, turb, solve')
    sys.exit(1)
if 'geom' not in cfg and 'geometry' not in cfg:
    print('ERROR: Config missing geometry section (geom or geometry)')
    sys.exit(1)
print('  ✓ Config valid')
" || exit 1

# 4. Quick MPI check (only if using multiple processes)
if [ "$NPROCS" -gt 1 ]; then
    mpirun -np 2 python -c "from mpi4py import MPI; print(f'  ✓ MPI rank {MPI.COMM_WORLD.rank}/{MPI.COMM_WORLD.size} OK')" 2>/dev/null || {
        echo "ERROR: MPI not working properly"
        exit 1
    }
fi

echo "Pre-flight checks passed!"
echo "───────────────────────────────────────────────────────────────────────"

# ─────────────────────────────────────────────────────────────────────────────
# Run solver
# ─────────────────────────────────────────────────────────────────────────────
cd "$SCRIPT_DIR"
if [ "$NPROCS" -gt 1 ]; then
    mpirun -np "$NPROCS" python -m dolfinx_rans "$CONFIG"
else
    python -m dolfinx_rans "$CONFIG"
fi

# Keep snapshots inside results directory (clean canonical layout).
OUT_DIR_ABS="$(python - <<PY
import json
from pathlib import Path

cfg_path = Path("$CONFIG")
cfg = json.loads(cfg_path.read_text())
out_dir = Path(cfg["solve"]["out_dir"])
if not out_dir.is_absolute():
    out_dir = (Path("$SCRIPT_DIR") / out_dir).resolve()
print(out_dir)
PY
)"

CANONICAL_SNPS_DIR="$OUT_DIR_ABS/results/snps"
LEGACY_SNPS_DIR="$(dirname "$OUT_DIR_ABS")/snps"
if [ -d "$LEGACY_SNPS_DIR" ] && [ "$LEGACY_SNPS_DIR" != "$CANONICAL_SNPS_DIR" ]; then
    mkdir -p "$CANONICAL_SNPS_DIR"
    shopt -s nullglob
    for item in "$LEGACY_SNPS_DIR"/*; do
        base="$(basename "$item")"
        rm -rf "$CANONICAL_SNPS_DIR/$base"
        mv "$item" "$CANONICAL_SNPS_DIR/$base"
    done
    shopt -u nullglob
    rmdir "$LEGACY_SNPS_DIR" 2>/dev/null || true
    echo "Moved legacy snapshots into: $CANONICAL_SNPS_DIR"
fi

# ─────────────────────────────────────────────────────────────────────────────
# Post-run regression checks (config-driven)
# ─────────────────────────────────────────────────────────────────────────────
python - <<PY || exit 1
import csv
import json
import math
import sys
from pathlib import Path

import numpy as np

cfg_path = Path("$CONFIG")
cfg = json.loads(cfg_path.read_text())
out_dir = Path(cfg["solve"]["out_dir"])
if not out_dir.is_absolute():
    out_dir = (Path("$SCRIPT_DIR") / out_dir).resolve()
results_dir = out_dir / "results"
re_tau = float(cfg["nondim"]["Re_tau"])
bench = dict(cfg.get("benchmark", {}))


def parse_bounds(val, key):
    if val is None:
        return None
    if not isinstance(val, (list, tuple)) or len(val) != 2:
        raise ValueError(f"benchmark.{key} must be a [min, max] list")
    lo, hi = float(val[0]), float(val[1])
    if not math.isfinite(lo) or not math.isfinite(hi) or lo > hi:
        raise ValueError(f"benchmark.{key} must satisfy finite min <= max")
    return lo, hi


def _extract_outer_profile(rows, re_tau):
    if not rows:
        raise ValueError("empty profile")
    header = rows[0]

    def _to_float(values, key):
        return np.array([float(r[key]) for r in values], dtype=float)

    # x-axis in outer coordinates y/δ
    y_over_delta = None
    if "y_over_delta" in header:
        y_over_delta = _to_float(rows, "y_over_delta")
    elif "y_lower_0_to_1" in header:
        y_over_delta = _to_float(rows, "y_lower_0_to_1")
    elif "y_plus" in header:
        # legacy wall-unit input
        y_re_tau = header.get("Re_tau", re_tau)
        y_re_tau = float(y_re_tau)
        if np.isfinite(y_re_tau) and y_re_tau > 0:
            y_over_delta = _to_float(rows, "y_plus") / y_re_tau
    if y_over_delta is None or not np.all(np.isfinite(y_over_delta)):
        raise ValueError("cannot determine y/δ coordinate from reference profile")

    # velocity as U/U_bulk
    u_over_ubulk = None
    if "u_over_ubulk" in header:
        u_over_ubulk = _to_float(rows, "u_over_ubulk")
    elif "u" in header:
        u_raw = _to_float(rows, "u")
        u_bulk = float(np.trapz(u_raw, y_over_delta) / max(y_over_delta[-1] - y_over_delta[0], 1e-30))
        u_over_ubulk = u_raw / max(u_bulk, 1e-30)
    elif "u_plus" in header:
        # For our RANS outputs u_plus == u, so normalize by the same bulk estimate.
        u_raw = _to_float(rows, "u_plus")
        u_bulk = float(np.trapz(u_raw, y_over_delta) / max(y_over_delta[-1] - y_over_delta[0], 1e-30))
        u_over_ubulk = u_raw / max(u_bulk, 1e-30)
    if u_over_ubulk is None:
        raise ValueError("reference profile missing velocity column (expected u_over_ubulk, u, or u_plus)")

    # k as k/U_bulk^2-equivalent for optional RMSE check
    k_over_ubulk = None
    if "k_over_ubulk" in header:
        k_over_ubulk = _to_float(rows, "k_over_ubulk")
    elif "k" in header:
        k_raw = _to_float(rows, "k")
        k_bulk = float(np.trapz(k_raw, y_over_delta) / max(y_over_delta[-1] - y_over_delta[0], 1e-30))
        k_over_ubulk = k_raw / max(k_bulk, 1e-30)
    elif "k_plus" in header:
        k_raw = _to_float(rows, "k_plus")
        k_bulk = float(np.trapz(k_raw, y_over_delta) / max(y_over_delta[-1] - y_over_delta[0], 1e-30))
        k_over_ubulk = k_raw / max(k_bulk, 1e-30)

    order = np.argsort(y_over_delta)
    return {
        "y": y_over_delta[order],
        "u": u_over_ubulk[order],
        "k": k_over_ubulk[order] if k_over_ubulk is not None else None,
    }


u_bounds = parse_bounds(bench.get("gate_u_bulk_bounds"), "gate_u_bulk_bounds")
tau_bounds = parse_bounds(bench.get("gate_tau_wall_bounds"), "gate_tau_wall_bounds")
ref_csv = bench.get("reference_profile_csv")
if isinstance(ref_csv, str) and not ref_csv.strip():
    ref_csv = None
u_rmse_max = bench.get("u_plus_rmse_max")
if u_rmse_max is None:
    u_rmse_max = bench.get("u_outer_rmse_max")
k_rmse_max = bench.get("k_plus_rmse_max")
if k_rmse_max is None:
    k_rmse_max = bench.get("k_outer_rmse_max")

if u_bounds is None and tau_bounds is None and ref_csv is None:
    print("Skipping regression gate (no benchmark thresholds configured).")
    sys.exit(0)

violations = []
summary_parts = []

if u_bounds is not None or tau_bounds is not None:
    history = results_dir / "history.csv"
    if not history.exists():
        print(f"ERROR: Regression gate failed: missing history file {history}")
        sys.exit(1)
    with history.open() as f:
        rows = list(csv.DictReader(f))
    if not rows:
        print(f"ERROR: Regression gate failed: empty history file {history}")
        sys.exit(1)
    last = rows[-1]
    u_bulk = float(last["U_bulk"])
    tau_wall = float(last["tau_wall"])
    summary_parts.append(f"U_bulk={u_bulk:.4f}")
    summary_parts.append(f"tau_wall={tau_wall:.4f}")

    if u_bounds is not None and not (u_bounds[0] <= u_bulk <= u_bounds[1]):
        violations.append(
            f"U_bulk={u_bulk:.4f} not in [{u_bounds[0]:.2f}, {u_bounds[1]:.2f}]"
        )
    if tau_bounds is not None and not (tau_bounds[0] <= tau_wall <= tau_bounds[1]):
        violations.append(
            f"tau_wall={tau_wall:.4f} not in [{tau_bounds[0]:.2f}, {tau_bounds[1]:.2f}]"
        )

if ref_csv is not None:
    profiles_path = results_dir / "profiles.csv"
    if not profiles_path.exists():
        print(f"ERROR: Regression gate failed: missing profile file {profiles_path}")
        sys.exit(1)

    ref_path = Path(ref_csv)
    if not ref_path.is_absolute():
        ref_path = (cfg_path.parent / ref_path).resolve()
    if not ref_path.exists():
        print(f"ERROR: Regression gate failed: reference profile not found {ref_path}")
        sys.exit(1)

    with profiles_path.open() as f:
        ours = list(csv.DictReader(f))
    with ref_path.open() as f:
        ref = list(csv.DictReader(f))
    if not ours:
        raise ValueError(f"{profiles_path} is empty")
    if not ref:
        raise ValueError(f"{ref_path} is empty")

    ours_parsed = _extract_outer_profile(ours, re_tau)
    ref_parsed = _extract_outer_profile(ref, re_tau)
    y_ours = ours_parsed["y"]
    u_ours = ours_parsed["u"]
    y_ref = ref_parsed["y"]
    u_ref = ref_parsed["u"]

    y_min = max(float(np.min(y_ours)), float(np.min(y_ref)))
    y_max = min(float(np.max(y_ours)), float(np.max(y_ref)))
    mask = (y_ref >= y_min) & (y_ref <= y_max)
    if int(np.count_nonzero(mask)) < 5:
        raise ValueError("Insufficient outer-profile overlap between computed and reference profiles")

    u_interp = np.interp(y_ref[mask], y_ours, u_ours)
    u_rmse = float(np.sqrt(np.mean((u_interp - u_ref[mask]) ** 2)))
    summary_parts.append(f"u_outer_rmse={u_rmse:.4f}")

    if u_rmse_max is not None and u_rmse > float(u_rmse_max):
        violations.append(
            f"u RMSE={u_rmse:.4f} exceeds limit {float(u_rmse_max):.4f}"
        )

    if k_rmse_max is not None:
        if ref_parsed["k"] is None:
            raise ValueError(f"{ref_path} missing k-related columns for k RMSE gate (k, k_plus, or k_over_ubulk)")
        if ours_parsed["k"] is None:
            raise ValueError(f"{profiles_path} missing k-related columns for k RMSE gate (k, k_plus, or k_over_ubulk)")
        k_interp = np.interp(y_ref[mask], y_ours, ours_parsed["k"])
        k_ref = ref_parsed["k"]
        k_rmse = float(np.sqrt(np.mean((k_interp - k_ref[mask]) ** 2)))
        summary_parts.append(f"k_rmse={k_rmse:.4f}")
        if k_rmse > float(k_rmse_max):
            violations.append(
                f"k RMSE={k_rmse:.4f} exceeds limit {float(k_rmse_max):.4f}"
            )

if violations:
    print("ERROR: Regression gate failed:")
    for item in violations:
        print(f"  - {item}")
    sys.exit(1)

details = ", ".join(summary_parts) if summary_parts else "no checks run"
print(f"Regression gate passed: {details}")
PY

echo "───────────────────────────────────────────────────────────────────────"
echo "Done! Check results in the '<out_dir>/results' directory."
